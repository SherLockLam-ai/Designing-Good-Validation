{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "brVR6w3Fyu8u"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset, SubsetRandomSampler\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms \n",
        "from torchvision.models import resnet18\n",
        "import torchvision \n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JQkzw4B70Wdb"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.model = resnet18(num_classes=10)\n",
        "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98UMB1eL0kSw",
        "outputId": "8e11f29f-4568-4978-f291-77f30d4fa124"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:42<00:00, 234kB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 48.1kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:03<00:00, 454kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.86MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_dataset = MNIST(download=True, root=\".\", transform=data_transform, train=True)\n",
        "test_dataset = MNIST(download=True, root=\".\", transform=data_transform, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eIF6WpUz1VOq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Function for computing the accuracy of the predictions over the entire data_loader\n",
        "'''\n",
        "def get_accuracy(model, data_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100*(correct/total)\n",
        "\n",
        "'''\n",
        "    Function for plotting training and validation losses\n",
        "'''\n",
        "def plot_losses(train_acc, valid_acc):\n",
        "    # change the style of the plots to seaborn\n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    train_acc = np.array(train_acc)\n",
        "    valid_acc = np.array(valid_acc)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "\n",
        "    ax.plot(train_acc, color=\"blue\", label=\"Train_acc\")\n",
        "    ax.plot(valid_acc, color=\"red\", label=\"Validation_acc\")\n",
        "    ax.set(title=\"Acc over epochs\",\n",
        "            xlabel=\"Epoch\",\n",
        "            ylabel=\"Acc\")\n",
        "    ax.legend()\n",
        "    fig.show()\n",
        "\n",
        "    # change the plot style to default\n",
        "    plt.style.use('default')\n",
        "\n",
        "'''\n",
        "    function for the training step of the training loop\n",
        "'''\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # backward and optimizer\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "    return model, optimizer, epoch_loss \n",
        "\n",
        "'''\n",
        "    function for the validation step of the training loop\n",
        "'''\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "\n",
        "    for images, labels in valid_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward pass and record loss\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(valid_loader)\n",
        "\n",
        "    return model, epoch_loss\n",
        "\n",
        "'''\n",
        "    function defining the entire training loop\n",
        "'''\n",
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
        "    # set object for storing metrics\n",
        "    best_loss = 1e10\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    list_train_acc = []\n",
        "    list_val_acc = []\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(0, epochs):\n",
        "        # training\n",
        "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
        "\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
        "\n",
        "        if epoch % print_every == print_every - 1:\n",
        "            train_acc = get_accuracy(model, train_loader, device=device)\n",
        "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
        "\n",
        "\n",
        "            print('Epochs: {}, Train_loss: {}, Valid_loss: {}, Train_accuracy: {}, Valid_accuracy: {}'.format(\n",
        "                    epoch, train_loss, valid_loss, train_acc, valid_acc\n",
        "                    ))\n",
        "\n",
        "            list_train_acc.append(train_acc)\n",
        "            list_val_acc.append(valid_acc)\n",
        "            train_losses.append(train_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "    plot_losses(list_train_acc, list_val_acc)\n",
        "\n",
        "    return model, optimizer, (train_losses, valid_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z3Z6JMpA1fyG"
      },
      "outputs": [],
      "source": [
        "dataset = ConcatDataset([train_dataset, test_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fqcAwZ4v2SJg"
      },
      "outputs": [],
      "source": [
        "def setup_dataflow(dataset, train_idx, val_idx):\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sMmbVo2h2BlG"
      },
      "outputs": [],
      "source": [
        "num_folds = 3\n",
        "splits = KFold(n_splits=num_folds,shuffle=True,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylzJPaGI3WJs",
        "outputId": "9589715e-94df-4a7a-a169-e760b493a392"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rrMhpWKT2no3",
        "outputId": "2fc652ce-3b48-4aba-abe4-bfc6ce8b4629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############### Fold 1 ###############\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.0001\u001b[39m)\n\u001b[32m      6\u001b[39m loss_function = nn.CrossEntropyLoss()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model, optimizer, _ = \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, epochs):\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     model, optimizer, train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, model, criterion, optimizer, device)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# backward and optimizer\u001b[39;00m\n\u001b[32m     62\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     optimizer.step()\n\u001b[32m     66\u001b[39m epoch_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fptsh\\Downloads\\AIO\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fptsh\\Downloads\\AIO\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fptsh\\Downloads\\AIO\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
        "  print('############### Fold {} ###############'.format(fold_idx + 1))\n",
        "  train_loader, val_loader = setup_dataflow(dataset, train_idx, val_idx)\n",
        "  model = Net().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  model, optimizer, _ = training_loop(model, loss_function, optimizer, train_loader, val_loader, 10, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
