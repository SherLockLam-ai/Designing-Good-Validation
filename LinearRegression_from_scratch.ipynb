{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bbPpZlTx0-nE"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "omj4rFGO2HYQ"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self, lr=0.001, iterations=1000):\n",
        "    self.lr = lr \n",
        "    self.iterations = iterations\n",
        "\n",
        "  def y_pred(self, X, w):\n",
        "    \"\"\"\n",
        "      w: weight tensor\n",
        "      X: input tensor\n",
        "    \"\"\"\n",
        "    # y = w^T * X\n",
        "    return torch.mm(torch.transpose(w, 0, 1), X) \n",
        "  \n",
        "  def loss(self, ypred, y):\n",
        "    \"\"\"\n",
        "      loss function - to measure the loss between estimated vs ground truth\n",
        "    \"\"\"\n",
        "    l = 1 / self.m * torch.sum(torch.pow(y-ypred, 2))\n",
        "\n",
        "  def gradient_descent(self, w, X, y, ypred):\n",
        "    \"\"\"\n",
        "      dCdW: derivative of cost function\n",
        "      w_update: change in weight tensor after each iteration\n",
        "    \"\"\"\n",
        "    dCdW = 2*self.m * torch.mm(X, torch.transpose(ypred-y, 0, 1))\n",
        "    w_update = w - self.lr * dCdW \n",
        "    \n",
        "    return w_update\n",
        "  \n",
        "  def run(self, X, y):\n",
        "    \"\"\"\n",
        "      type y: tensor object\n",
        "      type X: tensor object\n",
        "    \"\"\"\n",
        "    bias = torch.ones((1, X.shape[1]))\n",
        "    X = torch.cat((bias, X), dim=0)\n",
        "\n",
        "    self.m = X.shape[1]\n",
        "    self.n = X.shape[0]\n",
        "\n",
        "    w = torch.zeros((self.n, 1))\n",
        "\n",
        "    for iteration in range(1, self.iterations+1):\n",
        "      ypred = self.y_pred(X, w)\n",
        "      cost = self.loss(ypred, y)\n",
        "\n",
        "      if iteration % 100 == 0:\n",
        "        print(f'Loss at iteration {iteration} is {cost}')\n",
        "      \n",
        "      w = self.gradient_descent(w, X, y, ypred)\n",
        "    \n",
        "    return w "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RV9sF65_8Y69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at iteration 100 is None\n",
            "Loss at iteration 200 is None\n",
            "Loss at iteration 300 is None\n",
            "Loss at iteration 400 is None\n",
            "Loss at iteration 500 is None\n",
            "Loss at iteration 600 is None\n",
            "Loss at iteration 700 is None\n",
            "Loss at iteration 800 is None\n",
            "Loss at iteration 900 is None\n",
            "Loss at iteration 1000 is None\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  X: random initialization of input tensor\n",
        "  y: random initialization of input tensor\n",
        "\"\"\"\n",
        "X = torch.rand(1, 500)\n",
        "y = 2*X + 3 + torch.randn(1, 500) * 0.01\n",
        "\n",
        "model = LinearRegression(0.001, 1000)\n",
        "w = model.run(X, y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
